{
  "url": "https://api.github.com/repos/quarkusio/quarkus/issues/20951",
  "repository_url": "https://api.github.com/repos/quarkusio/quarkus",
  "labels_url": "https://api.github.com/repos/quarkusio/quarkus/issues/20951/labels{/name}",
  "comments_url": "https://api.github.com/repos/quarkusio/quarkus/issues/20951/comments",
  "events_url": "https://api.github.com/repos/quarkusio/quarkus/issues/20951/events",
  "html_url": "https://github.com/quarkusio/quarkus/issues/20951",
  "id": 1033648859,
  "node_id": "I_kwDOCFbutM49nDrb",
  "number": 20951,
  "title": "SRMSG18207 backpressure  Cannot emit item due to lack of requests",
  "labels": [
    {
      "id": 985346620,
      "node_id": "MDU6TGFiZWw5ODUzNDY2MjA=",
      "url": "https://api.github.com/repos/quarkusio/quarkus/labels/kind/bug",
      "name": "kind/bug",
      "color": "d73a4a",
      "default": false,
      "description": "Something isn't working"
    },
    {
      "id": 985346625,
      "node_id": "MDU6TGFiZWw5ODUzNDY2MjU=",
      "url": "https://api.github.com/repos/quarkusio/quarkus/labels/triage/invalid",
      "name": "triage/invalid",
      "color": "e4e669",
      "default": false,
      "description": "This doesn't seem right"
    },
    {
      "id": 1658790125,
      "node_id": "MDU6TGFiZWwxNjU4NzkwMTI1",
      "url": "https://api.github.com/repos/quarkusio/quarkus/labels/area/kafka",
      "name": "area/kafka",
      "color": "0366d6",
      "default": false,
      "description": ""
    }
  ],
  "state": "closed",
  "locked": false,
  "milestone": null,
  "comments": 9,
  "created_at": "2021-10-22T14:31:30Z",
  "updated_at": "2022-01-02T16:54:17Z",
  "closed_at": "2022-01-02T16:54:07Z",
  "active_lock_reason": null,
  "body": "### Describe the bug\n\nSRMSG18207: Unable to dispatch message to Kafka: io.smallrye.mutiny.subscription.BackPressureFailure: Cannot emit item due to lack of requests\r\n\r\nI have two batch streams consumbers and producers signature `Multi<Message<X>> process(Multi<Message<Y>> y)`\r\nthey both have some database work and there will be contention on the tables they both use\r\nThere is a massive backlog of messages for both topics when they start\r\n\r\nRunning either is normally fine but I was seeing occasional errors so I've added to the config:\r\n\r\n`mp.messaging.incoming.<stream>.max.poll.records=100`\r\n\r\nI occassionally see vertx blocked threads this doesn't appear to be fatal:\r\n\r\n```\r\nWARN  [io.ve.co.im.BlockedThreadChecker] (vertx-blocked-thread-checker) Thread Thread[vert.x-eventloop-thread-13,5,main] has been blocked for 8242 ms, time limit is 2000 ms: io.vertx.core.VertxException: Thread blocked\r\n```\r\n   \r\nbut eventually the whole thing stops and this does seem to be fatal:\r\n\r\n```\r\n\r\n14:50:16 ERROR [io.sm.re.me.kafka] (executor-thread-8) SRMSG18207: Unable to dispatch message to Kafka: io.smallrye.mutiny.subscription.BackPressureFailure: Cannot emit item due to lack of requests\r\n        at io.smallrye.mutiny.operators.multi.MultiBufferWithTimeoutOp$MultiBufferWithTimeoutProcessor.flushCallback(MultiBufferWithTimeoutOp.java:162)\r\n        at io.smallrye.mutiny.operators.multi.MultiBufferWithTimeoutOp$MultiBufferWithTimeoutProcessor.lambda$new$0(MultiBufferWithTimeoutOp.java:104)\r\n        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\r\n        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n        at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\r\n        at io.quarkus.vertx.core.runtime.VertxCoreRecorder$13.runWith(VertxCoreRecorder.java:548)\r\n        at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2449)\r\n        at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1478)\r\n        at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:29)\r\n        at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:29)\r\n        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n        at java.base/java.lang.Thread.run(Thread.java:834)\r\n```\r\n\r\n```\t\r\n14:50:16 ERROR [io.qu.mu.ru.MutinyInfrastructure] (vert.x-eventloop-thread-13) Mutiny had to drop the following exception: io.vertx.core.impl.NoStackTraceThrowable: Transaction already complete\r\n\r\n14:50:59 WARN  [io.sm.re.me.kafka] (vert.x-eventloop-thread-14) SRMSG18231: The amount of received messages without acking is too high for topic partition 'mytopic', amount 2. The last committed offset was 775. It means that the Kafka connector received Kafka Records that have neither be acked nor nacked in a timely fashion. The connector accumulates records in memory, but that buffer reached its maximum capacity or the deadline for ack/nack expired. The connector cannot commit as a record processing has not completed.\r\n```\r\n\r\n\r\nThis seems to coincide with a subscriber leaving the kafka server:\r\n`\r\n2021-10-22 10:51:00,213 - INFO  [data-plane-kafka-request-handler-3:Logging@66] - [GroupCoordinator 1]: Member[group.instance.id None, member.id kafka-consumer-<mytopic>-6e020433-73d7-46ab-b6b8-19ff68174717] in group 33dd1b61-caa8-439f-8631-81148dd1787f has left, removing it from the group`\r\n\r\nit never recovers\r\n\r\nI've tried adding the annotation below but it doesn't appear to make any difference:\r\n\r\n@Retry(delay=100, delayUnit=ChronoUnit.MILLIS, maxRetries=10)\r\n\r\n\r\n\r\nThe thread blocked messages look like this and there are several before it fails:\r\n\r\n```\r\n3,5,main] has been blocked for 2020 ms, time limit is 2000 ms: io.vertx.core.VertxException: Thread blocked\r\n        at java.base@11.0.9/java.util.Arrays.copyOf(Arrays.java:3689)\r\n        at java.base@11.0.9/java.util.concurrent.CopyOnWriteArrayList.add(CopyOnWriteArrayList.java:432)\r\n        at io.quarkus.arc.runtime.devconsole.Invocation$Builder.addChild(Invocation.java:165)\r\n        at io.quarkus.arc.runtime.devconsole.Invocation$Builder.newChild(Invocation.java:120)\r\n        at io.quarkus.arc.runtime.devconsole.InvocationTree.invocationStarted(InvocationTree.java:23)\r\n        at io.quarkus.arc.runtime.devconsole.InvocationTree_ClientProxy.invocationStarted(InvocationTree_ClientProxy.zig:138)\r\n        at io.quarkus.arc.runtime.devconsole.InvocationInterceptor.monitor(InvocationInterceptor.java:49)\r\n        at io.quarkus.arc.runtime.devconsole.InvocationInterceptor_Bean.intercept(InvocationInterceptor_Bean.zig:521)\r\n        at io.quarkus.arc.impl.InterceptorInvocation.invoke(InterceptorInvocation.java:41)\r\n        at io.quarkus.arc.impl.AroundInvokeInvocationContext.perform(AroundInvokeInvocationContext.java:41)\r\n        at io.quarkus.arc.impl.InvocationContexts.performAroundInvoke(InvocationContexts.java:32)\r\n        at io.quarkus.narayana.jta.runtime.CDIDelegatingTransactionManager_Subclass.getTransaction(CDIDelegatingTransactionManager_Subclass.zig:693)\r\n        at io.smallrye.context.jta.context.propagation.JtaContextProvider.currentTransaction(JtaContextProvider.java:102)\r\n        at io.smallrye.context.jta.context.propagation.JtaContextProvider.currentContext(JtaContextProvider.java:39)\r\n        at io.smallrye.context.impl.ThreadContextProviderPlan.takeThreadContextSnapshots(ThreadContextProviderPlan.java:72)\r\n        at io.smallrye.context.impl.SlowCapturedContextState.<init>(SlowCapturedContextState.java:25)\r\n        at io.smallrye.context.SmallRyeThreadContext.captureContext(SmallRyeThreadContext.java:729)\r\n        at io.smallrye.context.SmallRyeThreadContext.contextualConsumer(SmallRyeThreadContext.java:539)\r\n        at io.smallrye.mutiny.context.BaseContextPropagationInterceptor.decorate(BaseContextPropagationInterceptor.java:77)\r\n        at io.smallrye.mutiny.infrastructure.Infrastructure.decorate(Infrastructure.java:153)\r\n        at io.smallrye.mutiny.groups.UniOnItem.transformToUni(UniOnItem.java:168)\r\n        at io.smallrye.mutiny.groups.UniOnNull.failWith(UniOnNull.java:41)\r\n        at io.smallrye.mutiny.streams.stages.FlatMapCompletionStageFactory$FlatMapCompletionStage.lambda$apply$0(FlatMapCompletionStageFactory.java:52)\r\n        at io.smallrye.mutiny.streams.stages.FlatMapCompletionStageFactory$FlatMapCompletionStage$$Lambda$1006/0x0000000800afe040.apply(Unknown Source)\r\n        at io.smallrye.context.impl.wrappers.SlowContextualFunction.apply(SlowContextualFunction.java:21)\r\n        at io.smallrye.mutiny.groups.MultiOnItem.lambda$transformToUni$6(MultiOnItem.java:256)\r\n        at io.smallrye.mutiny.groups.MultiOnItem$$Lambda$1007/0x0000000800afe440.apply(Unknown Source)\r\n        at io.smallrye.mutiny.operators.multi.MultiFlatMapOp$FlatMapMainSubscriber.onItem(MultiFlatMapOp.java:176)\r\n        at io.smallrye.mutiny.subscription.MultiSubscriber.onNext(MultiSubscriber.java:61)\r\n        at io.smallrye.mutiny.subscription.SafeSubscriber.onNext(SafeSubscriber.java:98)\r\n        at io.smallrye.mutiny.helpers.HalfSerializer.onNext(HalfSerializer.java:31)\r\n        at io.smallrye.mutiny.helpers.StrictMultiSubscriber.onItem(StrictMultiSubscriber.java:83)\r\n        at io.smallrye.mutiny.operators.multi.MultiOperatorProcessor.onItem(MultiOperatorProcessor.java:67)\r\n        at io.smallrye.mutiny.operators.multi.MultiOnItemInvoke$MultiOnItemInvokeProcessor.onItem(MultiOnItemInvoke.java:41)\r\n        at io.smallrye.mutiny.operators.multi.MultiMapOp$MapProcessor.onItem(MultiMapOp.java:50)\r\n        at io.smallrye.mutiny.operators.multi.MultiOperatorProcessor.onItem(MultiOperatorProcessor.java:67)\r\n        at io.smallrye.mutiny.operators.multi.MultiOperatorProcessor.onItem(MultiOperatorProcessor.java:67)\r\n        at io.smallrye.mutiny.subscription.MultiSubscriber.onNext(MultiSubscriber.java:61)\r\n        at io.smallrye.mutiny.operators.uni.UniOnItemTransformToMulti$FlatMapPublisherSubscriber.onNext(UniOnItemTransformToMulti.java:59)\r\n        at io.smallrye.mutiny.helpers.HalfSerializer.onNext(HalfSerializer.java:31)\r\n        at io.smallrye.mutiny.helpers.StrictMultiSubscriber.onItem(StrictMultiSubscriber.java:83)\r\n        at io.smallrye.reactive.messaging.kafka.impl.KafkaRecordStream$KafkaRecordStreamSubscription.run(KafkaRecordStream.java:200)\r\n        at io.smallrye.reactive.messaging.kafka.impl.KafkaRecordStream$KafkaRecordStreamSubscription.lambda$dispatch$4(KafkaRecordStream.java:180)\r\n        at io.smallrye.reactive.messaging.kafka.impl.KafkaRecordStream$KafkaRecordStreamSubscription$$Lambda$1379/0x0000000800c3cc40.handle(Unknown Source)\r\n        at io.vertx.core.impl.AbstractContext.dispatch(AbstractContext.java:100)\r\n        at io.vertx.core.impl.AbstractContext.dispatch(AbstractContext.java:63)\r\n        at io.vertx.core.impl.EventLoopContext.lambda$runOnContext$0(EventLoopContext.java:38)\r\n        at io.vertx.core.impl.EventLoopContext$$Lambda$1077/0x0000000800b61440.run(Unknown Source)\r\n        at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\r\n        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n        at java.base@11.0.9/java.lang.Thread.run(Thread.java:834)\r\n\r\n```\r\n\r\nI've looked at https://quarkus.io/blog/mutiny-back-pressure/ and it I'm confused if this is a consumer or sender  problem, from the page it sounds like it's a sender problem but the timeouts indicate it's struggling to process the messages it's receiving, it doesn't sound like it's sending faster than kafka can receive them.\r\n\r\nI've no consumer for the output topic in quarkus I expect another processes to consume the messages.\r\n\r\nfrom the page I understand back pressure should be handled for me: \r\n\r\n> Mutiny implements the Reactive Streams protocol for you. In other words, when using Multi, you are using a Publisher following the Reactive Streams protocol. All the subscription handshakes and requests negotiations are done for you.\r\n\r\nThe fact this can fail after the one consumer has caught up is also very confusing as there would then be no database contention. Reducing the number of messages in the block processing seems to make it worse rather than better:\r\n\r\n`Multi<List<Message<SecurityAdditionalDetailValue>>> multiMessages = incoming.group().intoLists().of(100, Duration.ofSeconds(1));`\r\n\r\nDropping messages is not an option.\n\n### Expected behavior\n\nAll messages to be consumed and all new messages sent\n\n### Actual behavior\n\nmessage consumption ends abruptly with no recovery\r\n\r\nReally I'm hoping the error messages/stack traces lead some insight but I can look at creating a test case.\n\n### How to Reproduce?\n\nLarge backlog\r\n\r\nTwo slow consumers of messages\n\n### Output of `uname -a` or `ver`\n\nLinux JHC8700TP 5.10.16.3-microsoft-standard-WSL2 \n\n### Output of `java -version`\n\n17\n\n### GraalVM version (if different from Java)\n\n_No response_\n\n### Quarkus version or git rev\n\n2.3.0.Final\n\n### Build tool (ie. output of `mvnw --version` or `gradlew --version`)\n\napache-maven-3.8.1\n\n### Additional information\n\n_No response_",
  "reactions": {
    "url": "https://api.github.com/repos/quarkusio/quarkus/issues/20951/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/quarkusio/quarkus/issues/20951/timeline",
  "performed_via_github_app": null,
  "state_reason": "completed"
}
